{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6380e7",
   "metadata": {},
   "source": [
    "# LLM with Langchain and RAG using Mistrals API Key #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd0890",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "Install mistralai: The llm for use \n",
    "Install faiss-cpu: A library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "https://pypi.org/project/faiss-cpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07d18f",
   "metadata": {},
   "source": [
    "### Archictecture ###\n",
    "Load raw text  →  Split into chunks  →  Embed & index in ChromaDB\n",
    "                         ↓\n",
    "             At query time: Retrieve top-k chunks  →  \n",
    "             Build prompt  →  \n",
    "             Send to LLM  →  \n",
    "             Return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d525796",
   "metadata": {},
   "source": [
    "### Overview ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ff224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary frameworks/libraries and Langchain dependencies\n",
    "\n",
    "import bs4\n",
    "#BeautifulSoup reads raw text from a source/document\n",
    "from langchain import hub\n",
    "#Pull a pre-built prompt template\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#Breaks long documents into overlapping chunks\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "#\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#Chroma the vector store to store/save embeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#Langchain primites to help chain together 'retrieve->format->prompt->LLM->parse'\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "#Langchain primites to help chain together 'retrieve->format->prompt->LLM->parse'\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "#\n",
    "\n",
    "\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEXING\n",
    "loader = PyPDF2()\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d9b77",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "Create API in Mistral:\n",
    "https://console.mistral.ai/api-keys\n",
    "\n",
    "Embeddings and API in Mistral:\n",
    "https://docs.mistral.ai/api/#tag/embeddings\n",
    "\n",
    "Rag from Scratch:\n",
    "https://docs.mistral.ai/guides/rag/\n",
    "\n",
    "Text Embeddings:\n",
    "https://docs.mistral.ai/capabilities/embeddings/text_embeddings/\n",
    "\n",
    "Installing packages:\n",
    "https://pypi.org/project/bs4/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
